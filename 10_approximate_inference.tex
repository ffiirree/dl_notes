\chapter{Approximate Inference}

\section{Monte Carlo Methods}

\section{Variational Inference}

Consider a joint density of latent variables $z$ and observations $x$
\begin{equation}
    P(z, x) = P(z)P(x|z)
\end{equation}

In Bayesian models, the latent variables help govern the distribution of the data. A Bayesian
model draws the latent variables from a \textbf{prior density $P(z)$} and then relates them to the
observations through the \textbf{likelihood $P(x|z)$}. Inference in a Bayesian model amounts to
conditioning on data and computing the \textbf{posterior $P(z|x)$}. In complex Bayesian models,
this computation often requires approximate inference.

The inference problem is to compute the conditional density of the latent variables given the observations, $p(z|x)$. This conditional
can be used to produce point or interval estimates of the latent variables, form predictive densities of new data, and more. The conditional
density can be written as
\begin{equation}
    p(z|x) = \frac{p(z, x)}{p(x)}
\end{equation}
The denominator contains the marginal density of the observations, also called \textbf{evidence}, which can be calculated by marginalizing out
the latent variables from the joint density,
\begin{equation}
    p(x) = \int p(z, x) \d z
\end{equation}
For many models, this evidence integral is unavailable in closed form or requires exponential
time to compute.

\textbf{The main idea behind variational inference is to first posit a family of densities and then to find the member of that family which is close to
the target. Closeness is measured by Kullback-Leibler divergence.} Variational inference thus turns the
inference problem into an optimization problem.

First, we posit a family of approximate densities $\mathcal{Q}$. This is a set of densities over the latent
variables. Then, we try to find the member of that family that minimizes the KL divergence to the exact posterior,
\begin{equation}
    Q^*(z) = \arg \min_{Q(z) \in \mathcal{Q}} KL(Q(z)\|P(z|x))
\end{equation}
Finally, we approximate the posterior with the optimized member of the family $Q^*(z)$. The reach of the family $\mathcal{Q}$ manages the complexity of this
optimization. One of the key ideas behind variational inference is to choos $\mathcal{Q}$ to be flexible enough to capture a density close to $p(z|x)$, but
simple enough for efficient optimization.

However, this objective is not computable because it requires computing the evidence $\log p(x)$
\begin{equation}
    \begin{split}
        KL(Q(z)\|P(z|x))
        &= \E_{z \sim Q} [\log Q(z)] - \E_{z \sim Q} [\log P(z|x)] \\
        &= \E_{z \sim Q} [\log Q(z)] - \E_{z \sim Q} [\log P(z, x)] + \E_{z \sim Q}[\log P(x)]\\
        &= \E_{z \sim Q} [\log Q(z) - \log P(z, x)] + \log P(x) \\
        \\
        \log P(x)
        &= KL(Q(z)\|P(z|x)) + \E_{z \sim Q} [\log P(z, x) - \log Q(z)]
    \end{split}
\end{equation}

$\log P(x)$ is constant with respect to $Q(z)$, $KL(\cdot ) \ge 0$, then
\begin{equation}
    ELBO(Q) = \E_{z \sim Q} [\log P(z, x) - \log Q(z)] \le \log P(x)
\end{equation}

The function is call \textbf{evdience low bound(ELBO)}. Maximizing the ELBO is equivalent to
minimizing the KL divergence.

\begin{equation}
    \begin{split}
        ELBO(Q)
        &= \E_{z \sim Q} [\log P(z, x)] - \E_{z \sim Q} [\log Q(z)] \\
        &= \E_{z \sim Q} [\log P(z)] + \E_{z \sim Q} [\log P(x|z)] - \E_{z \sim Q} [\log Q(z)] \\
        &= \E_{z \sim Q} [\log P(x|z)] - KL(Q(z)\|P(z))
    \end{split}
\end{equation}

The first termis an expected likelihood; it encourages densities
that place their mass on configurations of the latent variables
that explain the observed data. The second term is the negative
divergence between the variational density and the prior; it encourages
densities close to the prior.

\subsection{Bayesian mixture of Gaussians}
Consider to Bayesian mixture of unit-variance univariate Gaussians. There are $K$ mixture components, corresponding to $K$ Gaussian
distributions with means $\mu = \{\mu_1,...,\mu_k\}$. The mean parameters are drawn independently from a common prior $p(\mu_k)$, which
we assume to be as Gaussian $\N(0, \sigma^2)$; the prior variance $\sigma^2$ is a hyperparameter. To generate an observation $x_i$ comes
from the model, we first choose a cluster assignment $c_i$. It indicates which latent cluster $x_i$ comes from and is drawn from a categorical
distribution over $\{1,...K\}$.(We encode $c_i$ as an indicator $K$-vector, all zeros except for a one in the position corresponding to $x_i$'s cluster.)
We then draw $x_i$ from the corresponding Gaussian $\N(c_i^T\mu, 1)$.

The full hierarchical model is:
\begin{equation}
    \begin{split}
        \mu_k &\sim \N(0, \sigma^2), \\
        c_i &\sim Categorical(\frac{1}{K}, ..., \frac{1}{K}), \\
        x_i|c_i, \mu &\sim \N(c_i^T\mu, 1)
    \end{split}
\end{equation}
