\chapter{Skip Connections}

\section{Residual Units}
为了训练deeper model，并解决梯度消失/爆炸以及网络degradation problem，在ResNets\cite{He2016resnet}中提出，如果在shallower model
中添加identity mapping，那么deeper model理应不会有更高的错误率。
\par
Residual unit表示如下:
\begin{equation}
    \begin{split}
        \vy_l &= h(\vx_l) + \mathcal{F}(\vx_l, \mathcal{W}_l) \\
        \vx_{l+1} &= f(\vy_l)
    \end{split}
\end{equation}
In ResNet，$h(\vx_) = \vx_l$ is an identity mapping and $f$ is a ReLU function.
\par
在ResNet中，因为$f$不是identity mapping，所以残差只能ResNet units中学习且信息不能直接传达到后面的层中，
In \cite{He2016identity}，希望\textit{propagating information}可以\textit{through theentire network}.
\begin{quotation}
    Our derivations reveal that \textit{if both $h(\vx_l)$ and
$f(\vy_l)$ are identity mappings}, the signal could be \textit{directly}
propagated from one unit to any other units, in both forward and backward passes.\cite{He2016identity}
\end{quotation}

if $f$ is also an identity mapping:
\begin{equation}
    \begin{split}
        \vx_{l+1} = \vx_l + \mathcal{F}(\vx_l, \mW_l)
    \end{split}
\end{equation}
then we will have:
\begin{equation}
    \begin{split}
        \vx_{l+n} &= \vx_{l + n - 1} + \mathcal{F}(\vx_{l + n -1}, \mathcal{W}_{l + n -1}) \\
        &= \vx_{l + n - 2} + \mathcal{F}(\vx_{l + n - 2}, \mathcal{W}_{l + n - 2}) + \mathcal{F}(\vx_{l + n -1}, \mathcal{W}_{l + n -1})\\
        &= \vx_l + \sum_{i=l}^{l+n-1} \mathcal{F}(\vx_i, \mathcal{W}_i)
    \end{split}
\end{equation}
相较于plain network(ignoring BN and ReLU is an identity mapping):
\begin{equation}
    \begin{split}
        \vx_{l+n} &= \prod_{i=l}^{l+n-1} \mW_i \vx_i
    \end{split}
\end{equation}
反向传播过程:
\begin{equation}
    \begin{split}
        \frac{\partial \E}{\partial \vx_l}
        &= \frac{\partial \E}{\partial \vx_{l+n}} \frac{\partial \vx_{l+n}}{\partial \vx_l} \\
        &= \frac{\partial \E}{\partial \vx_{l+n}} \Bigg (1 + \frac{\partial}{\partial \vx_{l}}\sum_{i=l}^{l+n-1} \mathcal{F}(\vx_i, \mathcal{W}_i)\Bigg)
    \end{split}
\end{equation}
可以看出，上式可以分为两部分，$\frac{\partial \E}{\partial \vx_{l+n}}$不经过任何权重信息，可以直接传播到浅层，
只要括号中的第二部分不总是为$-1$，那么\textbf{即使权重任意小也不会发生梯度消失}。

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/residual_units.png}
    \caption{Residual Units}
    \label{fig:residual_units}
\end{figure}

以上的分析基于$f$是一个恒等变换，但实际上$f$会影响之前分析的两条信息传播的路径:
\begin{equation}
    \vx_{l+1} = f(\vx_l) + \mathcal{F}(f(\vx_l), \mathcal{W}_l)
\end{equation}
因此，在\cite{He2016identity}提出了新的Residual unit结构pre-activation，等式变为
\begin{equation}
    \vx_{l+1} = \vx_l + \mathcal{F}(\hat f(\vx_l), \mathcal{W}_l)
\end{equation}

\section{The Shattered Gradients Problem}
\begin{quotation}
    A previously unnoticed difficulty with gradients in deep rectifier networks that orthogonal
    to vanishing and exploding gradients. The shattering gradients problem is that, as depth increases,
    \textbf{gradients in standard feedforward networks increasingly resemble white noise}\cite{Balduzzi2017}.
\end{quotation}
\begin{itemize}
    \item Gradients of shallow networks resemble brown noise(布朗噪声).
    \item Gradients of deep networks resemble white noise(白噪声).
    \item Training is difficult when gradients behave like white noise.
    \item Gradients of deep resnets lie in between brown and white noise.
\end{itemize}

在标准前馈神经网络中，神经元相关性按指数级减少($\frac{1}{2^L}$),同时，梯度的空间结构也随着深度增加被逐渐消除。
使用BatchNorm的ResNets中梯度相关系减少的速度从指数级减少到亚线性级($\frac{1}{\sqrt(L)}$)，极大的保留梯度的空间结构。

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/brown_white_noise.png}
    \caption{The shattered Gradients Problem}
    \label{fig:brown_white_noise}
\end{figure}


\section{DenseNet}
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/densenet.png}
    \caption{DenseNet}
    \label{fig:densenet}
\end{figure}

\section{FCN}
\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/fcns.png}
    \caption{FCN}
    \label{fig:fcn}
\end{figure}

\section{UNet family}
\subsection{U-Net}

\subsubsection{Architecture}
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{images/unet.png}
    \caption{UNet}
    \label{fig:unet}
\end{figure}
UNet\cite{UNet2015}: The architecture consists of a \textbf{contracting path} to capture
context and a symmetric \textbf{expanding path} that enables precise localization.

[offical implementation]\url{http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net}

\subsubsection{Metrics}

\subsection{VNet}

\subsubsection{Dice coeffcient}
\[
dice = \frac{2|X \cap Y |}{|X| + |Y|}
\]


\subsection{UNet 2+}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/unet2+.png}
    \caption{UNet 2+}
    \label{fig:unet2p}
\end{figure}

多深合适？降采样对分割网络到底是不是必须的？
不一定要降到第四次才上采样，使用浅层和深层的特征。

\subsection{UNet 3+}

\begin{figure}[H]
    \centering
    \includegraphics[width=14cm]{images/unet3+.png}
    \caption{UNet 3+}
    \label{fig:unet3p}
\end{figure}
都缺乏从全尺度探索足够信息的能力，未能明确了解器官的位置和边界。UNet 3+中的每一个解码器层都融合了来自编码器中的小尺度和同尺度的特征图，
以及来自解码器的大尺度的特征图，这些特征图捕获了全尺度下的细粒度语义和粗粒度语义。
