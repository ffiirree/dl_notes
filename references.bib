@inproceedings{Glorot2010,
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. Copyright 2010 by the authors.},
  author   = {Xavier Glorot and Yoshua Bengio},
  issn     = {15324435},
  journal  = {Journal of Machine Learning Research},
  pages    = {249-256},
  title    = {Understanding the difficulty of training deep feedforward neural networks},
  volume   = {9},
  year     = {2010}
}
@article{He2015,
  abstract      = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1502.01852},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  doi           = {10.1109/ICCV.2015.123},
  eprint        = {1502.01852},
  file          = {:C$\backslash$:/Users/ffiiirree/OneDrive/Papers/Basic/Activation functions {\&} Initialization schemes/201502{\_}[PReLU, Kaiming]Delving Deep into Rectifiers. Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
  isbn          = {9781467383912},
  issn          = {15505499},
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  pages         = {1026--1034},
  title         = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
  volume        = {2015 International Conference on Computer Vision, ICCV 2015},
  year          = {2015}
}
@article{Cybenko1989,
  abstract  = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks. {\textcopyright} 1989 Springer-Verlag New York Inc.},
  author    = {Cybenko, G.},
  doi       = {10.1007/BF02551274},
  issn      = {09324194},
  journal   = {Mathematics of Control, Signals, and Systems},
  keywords  = {Approximation,Completeness,Neural networks},
  month     = {dec},
  number    = {4},
  pages     = {303--314},
  publisher = {Springer-Verlag},
  title     = {{Approximation by superpositions of a sigmoidal function}},
  volume    = {2},
  year      = {1989}
}

@article{Leshno1993,
  abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold. {\textcopyright} 1993 Pergamon Press Ltd.},
  author   = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  doi      = {10.1016/S0893-6080(05)80131-5},
  issn     = {08936080},
  journal  = {Neural Networks},
  number   = {6},
  title    = {{Multilayer feedforward networks with a nonpolynomial activation function can approximate any function}},
  volume   = {6},
  year     = {1993}
}
@inproceedings{He2016identity,
  abstract  = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR- 10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet.},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi       = {10.1007/978-3-319-46493-0_38},
  issn      = {16113349},
  title     = {{Identity mappings in deep residual networks}},
  volume    = {9908 LNCS},
  year      = {2016}
}
@inproceedings{He2016resnet,
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {10636919},
  title     = {{Deep residual learning for image recognition}},
  volume    = {2016-December},
  year      = {2016}
}

@inproceedings{Long2015,
  abstract  = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  doi       = {10.1109/CVPR.2015.7298965},
  issn      = {10636919},
  title     = {{Fully convolutional networks for semantic segmentation}},
  volume    = {07-12-June-2015},
  year      = {2015}
}

@book{zhang2020dive,
  title  = {Dive into Deep Learning},
  author = {Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
  note   = {\url{https://d2l.ai}},
  year   = {2020}
}

@inproceedings{Balduzzi2017,
  abstract  = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite wellchosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new "looks linear" (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.},
  author    = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan Duo and McWilliams, Brian},
  booktitle = {34th International Conference on Machine Learning, ICML 2017},
  title     = {{The shattered gradients problem: If resnets are the answer, then what is the question?}},
  volume    = {1},
  year      = {2017}
}

@inproceedings{Ioffe2015,
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
  author    = {Ioffe, Sergey and Szegedy, Christian},
  booktitle = {32nd International Conference on Machine Learning, ICML 2015},
  title     = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
  volume    = {1},
  year      = {2015}
}

@inproceedings{Santurkar2018,
  abstract  = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called “internal covariate shift”. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  author    = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle = {Advances in Neural Information Processing Systems},
  issn      = {10495258},
  title     = {{How does batch normalization help optimization?}},
  volume    = {2018-December},
  year      = {2018}
}
