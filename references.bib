@inproceedings{Glorot2010,
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. Copyright 2010 by the authors.},
  author   = {Xavier Glorot and Yoshua Bengio},
  issn     = {15324435},
  journal  = {Journal of Machine Learning Research},
  pages    = {249-256},
  title    = {Understanding the difficulty of training deep feedforward neural networks},
  volume   = {9},
  year     = {2010}
}
@article{He2015,
  abstract      = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1502.01852},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  doi           = {10.1109/ICCV.2015.123},
  eprint        = {1502.01852},
  file          = {:C$\backslash$:/Users/ffiiirree/OneDrive/Papers/Basic/Activation functions {\&} Initialization schemes/201502{\_}[PReLU, Kaiming]Delving Deep into Rectifiers. Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
  isbn          = {9781467383912},
  issn          = {15505499},
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  pages         = {1026--1034},
  title         = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
  volume        = {2015 International Conference on Computer Vision, ICCV 2015},
  year          = {2015}
}
@article{Cybenko1989,
  abstract  = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks. {\textcopyright} 1989 Springer-Verlag New York Inc.},
  author    = {Cybenko, G.},
  doi       = {10.1007/BF02551274},
  issn      = {09324194},
  journal   = {Mathematics of Control, Signals, and Systems},
  keywords  = {Approximation,Completeness,Neural networks},
  month     = {dec},
  number    = {4},
  pages     = {303--314},
  publisher = {Springer-Verlag},
  title     = {{Approximation by superpositions of a sigmoidal function}},
  volume    = {2},
  year      = {1989}
}

@article{Leshno1993,
  abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold. {\textcopyright} 1993 Pergamon Press Ltd.},
  author   = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  doi      = {10.1016/S0893-6080(05)80131-5},
  issn     = {08936080},
  journal  = {Neural Networks},
  number   = {6},
  title    = {{Multilayer feedforward networks with a nonpolynomial activation function can approximate any function}},
  volume   = {6},
  year     = {1993}
}
