\chapter{Deep Generative Models}

\begin{itemize}
    \item \textbf{Generative} models can capture the joint probability $P(X, Y)$, or just $P(x)$ if there are no labels
    \item \textbf{Discriminative} models capture the conditional probability $P(Y|X)$
\end{itemize}

\section{Auto-Regressive Generative Models}
PixelRNNs and PixelCNNs model the joint distribution of pixels over an image $x$ as the following
product of conditional distributions.
\begin{equation}
    P(x) = \prod^{n^2}_{i=1}P(x_{i}|x_1,...,x_{i-1})
\end{equation}

\section{Variational Autoencoders}
\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/vae.png}
    \caption{vae}
    \label{fig:VAE}
\end{figure}
\subsection{Why do we need VAEs?}

Autoencoder的encoder生成的latent space不够regurlarity(不连续)，我们无法从latent space的中随机采样来生成或修改图片


The lack of interpretable and exploitable structures in the latent space (\textbf{lack of regularity}).
the regularity of the latent space for autoencoders is a difficult point that depends on the distribution of the data in the initial space, the dimension of the latent space and the architecture of the encoder. So, it is pretty difficult (if not impossible) to ensure, a priori, that the encoder will organize the latent space in a smart way compatible with the generative process we just described.

\begin{figure}[H]
    \centering
    \includegraphics[width=8cm]{images/mnist_ae_latent_space.png}
    \caption{the encodings from a 2D latent space / MNIST}
    \label{fig:mnist_ae_latent_space}
\end{figure}

\subsection{VAE}

A variational autoencoder can be defined as being an autoencoder whose training is
regularised to avoid overfitting and ensure that the latent space has good properties
that enable generative process.

VAE模型需要符合以下条件
\begin{itemize}
    \item the input is \textbf{encoded as distribution} over the latent space, instead of encoding an input as a single point
    \item a point from the latent space is sampled from that distribution
    \item the sampled point is decoded and the reconstruction error can be computed
    \item the reconstruction error is backpropagated through the network
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/ae_vae.png}
    \label{fig:AEvsVAE}
\end{figure}

Latent space应该具有以下的性质
\begin{itemize}
    \item \textbf{Continuity} two close points in the latent space should not give two completely different contents once decoded
    \item \textbf{Completeness} for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/irregular_latent_space.png}
    \label{fig:irregular_latent_space}
\end{figure}
The only fact that VAEs encode inputs as distributions instead of simple points is
not sufficient to ensure continuity and completeness. Without a well defined
regularisation term, the model can learn, in order to minimise its reconstruction
error, to “ignore” the fact that distributions are returned and behave almost like
classic autoencoders (leading to overfitting). To do so, the encoder can either return
distributions with tiny variances (that would tend to be punctual distributions(点分布)) or return
distributions with very different means (that would then be really far apart from each other
in the latent space).
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/regularised_distributions_vae.png}
    \label{fig:regularised_distributions_vae}
\end{figure}
we have to regularise both the covariance matrix and the mean of the distributions
returned by the encoder. In practice, this regularisation is done by \textbf{enforcing
distributions to be close to a standard normal distribution}
\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/vae_regularisation.png}
    \label{fig:regularised_distributions_vae}
\end{figure}

\subsection{The VAE Lower Bound}
We approximate the posterior with the optimized member of the family $Q^*(z|x)$.
\begin{equation}
    Q_{\phi}^*(z|x) = \arg \min_{Q_{\phi}(z|x) \in \mathcal{Q}} KL(Q_{\phi}(z|x)\|P_{\theta}(z|x))
\end{equation}

That is equivalent to ELBO:

\begin{equation}
    \mathcal{L} = -ELBO(\theta, \phi, x) = KL(Q_{\phi}(z|x)\|P(z)) - \E_{z \sim Q_{\phi}(z|x)} [\log P_{\theta}(x|z)]
\end{equation}

\subsection{Regularizer - Solution of $KL(Q(z)\|P(z))$, Gaussion case}

VAEs take an unusual approach to dealing with this problem: they assume that there is no simple interpretation
of the dimensions of z, and instead assert that samples of z can be drawn from a simple distribution, i.e.,
$\N(0, I)$, where $I$ is the identity matrix. The key is to notice that any distribution in $d$ dimensions
can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently
complicated function.

Let the prior over the latent variables be the \textbf{centered isotropic multivariate Gaussian} $P(z) = \N (z; 0, I)$. In this case, we can
let the variational approximate posterior be a \textbf{multivariate Gaussian} with a \textbf{diagonal covariance structure}
- $Q_{\phi}(z|x) = \N (z; \mu(x; \phi), \Sigma(x; \phi))$.

\begin{equation}
    \begin{split}
        KL(Q_{\phi}(z|x)\|P(z))
        &= KL(\N(\mu(x), \Sigma(x))\|\N(0, I)) \\
        &= \frac{1}{2}\left(\mu(x)^\intercal\mu(x)  + \tr (\Sigma(x)) - k - \log \det (\Sigma(x))\right)
    \end{split}
\end{equation}

\subsection{Reconstruction Error}
We want to \textbf{differentiate} and optimize the lower bound $ELBO(\theta, \phi, x)$ w.r.t both the variational
parameters $\phi$ and generativet parameters $\theta$.由于隐变量$z$从$Q(z|x)$的采样过程不可微，所以需要改写成一个可微的形式.
we can sample from
$\N(\mu(x), \Sigma(x))$ by first sampling $\epsilon \sim \N(0, I)$, then computing
$z = \mu(x) + \Sigma^{\frac{1}{2}}(x) * \epsilon$ - \textbf{reparameterization trick}.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{images/vae_reparameterization_trick.png}
    \label{fig:vae_reparameterization_trick}
\end{figure}

\section{Generative Adversarial Models}

\subsection{Mode collapse}
https://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/

\subsection{Wasserstein GAN and the Kantorovich-Rubinstein Duality}
https://vincentherrmann.github.io/blog/wasserstein/

\subsection{vanilla GAN}
D and G play the following two-player minimax game with value function $V(G,D)$:
\begin{equation}
    \min_G \max_D V(D, G)
    = \E_{x \sim P(x)}[\log D(x)] + \E_{z \sim P(z)}[\log (1 - D(G(z)))]
\end{equation}

\textbf{The training criterion for the discriminator D, given any generator G, is to maximize the
quantity $V(G, D)$}
\begin{equation}
    \begin{split}
        V(G, D)
        &= \E_{x \sim P(x)}[\log D(x)] + \E_{z \sim P(z)}[\log (1 - D(G(z)))] \\
        &= \E_{x \sim P_d(x)}[\log D(x)] + \E_{x \sim P_g(x)}[\log (1 - D(x))]
    \end{split}
\end{equation}

For any $(a, b) \in \mathbb{R}^2 \backslash \{0, 0\}$, the function $ y \to a\log(y) + b\log(1 - y)$
achieves its maximum in [0, 1] at $\frac{a}{a + b}$.
So, for G fixed, the optimal discriminator D is
\begin{equation}
    D_{G}^{*}(x) = \frac{P_d(x)}{P_d(x) + P_g(x)}
\end{equation}

Note that the training objective for D can be interpreted as maximizing the log-likelihood for
estimating the conditional probability $P(Y = y|x), x \in P_d or \in P_g$, then
\begin{equation}
    \begin{split}
        C(G) &= max_D V(G, D) \\
        &= \E_{x \sim P_d(x)}[\log D_G^*(x)] + \E_{z \sim P(z)}[\log(1 - D_G^*(G(z)))] \\
        &= \E_{x \sim P_d(x)}[\log D_G^*(x)] + \E_{x \sim P_g(x)}[\log D_G^*(x)] \\
        &= \E_{x \sim P_d(x)}[\log \frac{P_d(x)}{P_d(x) + P_g(x)}] + \E_{x \sim P_g(x)}[\log \frac{P_g(x)}{P_d(x) + P_g(x)}]
    \end{split}
\end{equation}

\textbf{The global minimum of C(G) is achieved if and onl if $P_g = P_d$.} And
\begin{equation}
    C(G) = - \log 4
\end{equation}
and that by substracting this expression from $C(G) = V(D_G^*, G)$, we abtain:
\begin{equation}
    \begin{split}
        C(G) &= -log(4) + KL(P_d\| \frac{P_d + P_g}{2}) + KL(P_g \| \frac{P_d + P_g}{2}) \\
        &= -2\log(2) + 2JSD(P_d\|P_g)
    \end{split}
\end{equation}
在D最优的情况下，等价于优化JSD

\subsection{Latent space}
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{images/cgan_acgan.png}
    \label{fig:CGAN2ACGAN}
\end{figure}
\subsubsection{Conditional GAN}
使用辅助信息——label，将label和image绑定，分别输入到D和G中，
\begin{equation}
    \min_G \max_D V(D, G) = \E_{x \sim P_d(x)}[\log D(x|y)] + \E_{z \sim P_{z}(z)}[\log (1 - D(G(z|y)))]
\end{equation}

\textbf{可以使用标签控制生成指定类型的图片}
\subsubsection{Semi-Supervised GAN}


\subsubsection{InfoGAN}
\begin{equation}
    \min_G \max_D V_I(D, G) = V(D, G) - \lambda I(c; G(z,c))
\end{equation}

\subsubsection{ACGAN}

\subsection{Architecture}

\subsection{Object functions}

